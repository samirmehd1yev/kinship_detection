{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries and Setup\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from PIL import Image\n",
    "\n",
    "# InsightFace\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# Initialize Logging\n",
    "logging.basicConfig(\n",
    "    filename='feature_extraction_pytorch.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# 2. Load the InsightFace Model (buffalo_l)\n",
    "def initialize_insightface():\n",
    "    \"\"\"\n",
    "    Initialize the buffalo_l model pack which includes ArcFace.\n",
    "    \"\"\"\n",
    "    model_pack_name = 'buffalo_l'\n",
    "    app = FaceAnalysis(name=model_pack_name)\n",
    "    app.prepare(ctx_id=0, det_size=(128, 128))\n",
    "    return app\n",
    "\n",
    "# Initialize the model\n",
    "app = initialize_insightface()\n",
    "logging.info(\"buffalo_l model loaded successfully.\")\n",
    "\n",
    "\n",
    "# 3. Define Feature Extraction Function\n",
    "def extract_features_insightface(img_path, model):\n",
    "    \"\"\"\n",
    "    Extract feature vector from an image using the ArcFace model in the buffalo_l pack.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image file.\n",
    "        model: Loaded FaceAnalysis model with ArcFaceONNX recognition.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Extracted feature vector or None if extraction fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and convert the image\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = np.array(img)  # Convert to numpy array\n",
    "\n",
    "        # Use the model to detect and recognize the face\n",
    "        faces = model.get(img)  # This will detect faces and get embeddings\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            # Extract the 512-dimensional feature embedding of the first detected face\n",
    "            face_embedding = faces[0].embedding\n",
    "            return face_embedding.flatten()  # Return a 1D feature vector\n",
    "        else:\n",
    "            logging.info(f\"No face detected in {img_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error processing {img_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the features\n",
    "print(f\"Shape of anchor features: {anchors_features.shape}\")\n",
    "print(f\"Shape of positive features: {positives_features.shape}\")\n",
    "print(f\"Shape of negative features: {negatives_features.shape}\")\n",
    "print(f\"Shape of labels: {labels.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinship_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "RELATIONSHIP_TYPES = ['ss', 'bb', 'ms', 'fs', 'fd', 'md', 'sibs', 'gfgs', 'gmgs', 'gfgd', 'gmgd']\n",
    "RELATIONSHIP_TO_IDX = {rel: idx for idx, rel in enumerate(RELATIONSHIP_TYPES)}\n",
    "INPUT_SHAPE = (112, 112, 3)  # Correct image size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Age difference expectations for relationships (in years)\n",
    "AGE_EXPECTATIONS = {\n",
    "    'ss': 5,     # Allow some age difference for sisters\n",
    "    'bb': 5,     # Allow some age difference for brothers\n",
    "    'ms': 25,    # Mother-son age difference\n",
    "    'fs': 30,    # Father-son age difference\n",
    "    'fd': 30,    # Father-daughter age difference\n",
    "    'md': 25,    # Mother-daughter age difference\n",
    "    'sibs': 5,   # Siblings age difference\n",
    "    'gfgs': 55,  # Grandfather-grandson age difference\n",
    "    'gmgs': 50,  # Grandmother-grandson age difference\n",
    "    'gfgd': 55,  # Grandfather-granddaughter age difference\n",
    "    'gmgd': 50   # Grandmother-granddaughter age difference\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinshipFeatureExtractor(layers.Layer):\n",
    "    def __init__(self, name=\"feature_extractor\"):\n",
    "        super(KinshipFeatureExtractor, self).__init__(name=name)\n",
    "        \n",
    "        # Initialize ResNet50 with correct input shape\n",
    "        base_model = tf.keras.applications.ResNet50V2(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=INPUT_SHAPE\n",
    "        )\n",
    "        \n",
    "        # Freeze early layers\n",
    "        for layer in base_model.layers[:100]:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        self.base_model = base_model\n",
    "        \n",
    "        # Additional layers\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.dropout1 = layers.Dropout(0.5)\n",
    "        self.dense1 = layers.Dense(512, activation='relu')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.dropout2 = layers.Dropout(0.3)\n",
    "        self.dense2 = layers.Dense(256)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.base_model(x, training=training)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        return tf.nn.l2_normalize(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinshipModel(Model):\n",
    "    def __init__(self, margin=0.3, name=\"kinship_model\"):\n",
    "        super(KinshipModel, self).__init__(name=name)\n",
    "        self.margin = margin\n",
    "        self.feature_extractor = KinshipFeatureExtractor()\n",
    "        \n",
    "        # Relationship classifier\n",
    "        self.classifier = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(len(RELATIONSHIP_TYPES))\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        anchor, positive, negative = inputs\n",
    "        \n",
    "        # Get embeddings\n",
    "        anchor_embedding = self.feature_extractor(anchor, training=training)\n",
    "        positive_embedding = self.feature_extractor(positive, training=training)\n",
    "        negative_embedding = self.feature_extractor(negative, training=training)\n",
    "        \n",
    "        # Compute distances\n",
    "        positive_distance = tf.reduce_sum(tf.square(anchor_embedding - positive_embedding), axis=1)\n",
    "        negative_distance = tf.reduce_sum(tf.square(anchor_embedding - negative_embedding), axis=1)\n",
    "        \n",
    "        # Classification logits from positive pair\n",
    "        combined_features = tf.concat([anchor_embedding, positive_embedding], axis=1)\n",
    "        classification_output = self.classifier(combined_features, training=training)\n",
    "        \n",
    "        return positive_distance, negative_distance, classification_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    \"\"\"Combined triplet and classification loss\"\"\"\n",
    "    positive_distance, negative_distance, classification_output = y_pred\n",
    "    relationship_labels = y_true\n",
    "    \n",
    "    # Triplet loss\n",
    "    margin = 0.3\n",
    "    triplet_loss = tf.maximum(0., positive_distance - negative_distance + margin)\n",
    "    \n",
    "    # Classification loss\n",
    "    classification_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        relationship_labels, \n",
    "        tf.nn.softmax(classification_output), \n",
    "        from_logits=False\n",
    "    )\n",
    "    \n",
    "    # Combine losses (weighted sum)\n",
    "    total_loss = triplet_loss + 0.5 * classification_loss\n",
    "    \n",
    "    return tf.reduce_mean(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess image\"\"\"\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    # No need to resize as images are already 112x112\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Simple normalization\n",
    "    return image\n",
    "\n",
    "def create_dataset(df, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Create dataset from DataFrame containing triplets\"\"\"\n",
    "    def generator():\n",
    "        for _, row in df.iterrows():\n",
    "            # Load and preprocess images\n",
    "            anchor = load_and_preprocess_image(row['Anchor'])\n",
    "            positive = load_and_preprocess_image(row['Positive'])\n",
    "            negative = load_and_preprocess_image(row['Negative'])\n",
    "            \n",
    "            # Get label (relationship type)\n",
    "            label = RELATIONSHIP_TO_IDX[row['ptype']]\n",
    "            \n",
    "            yield (anchor, positive, negative), label\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=INPUT_SHAPE, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=INPUT_SHAPE, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=INPUT_SHAPE, dtype=tf.float32)\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "        )\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinshipTrainer:\n",
    "    def __init__(self, model, train_dataset, val_dataset, test_dataset):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.reset_metrics()\n",
    "        \n",
    "        # For tracking history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Initialize/Reset metrics\"\"\"\n",
    "        self.train_loss_values = []\n",
    "        self.val_loss_values = []\n",
    "        self.train_accuracy_values = []\n",
    "        self.val_accuracy_values = []\n",
    "        \n",
    "        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(images, training=True)\n",
    "            loss = combined_loss(labels, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        \n",
    "        # Update accuracy\n",
    "        self.train_accuracy.update_state(labels, tf.nn.softmax(predictions[2]))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def val_step(self, images, labels):\n",
    "        predictions = self.model(images, training=False)\n",
    "        loss = combined_loss(labels, predictions)\n",
    "        \n",
    "        # Update accuracy\n",
    "        self.val_accuracy.update_state(labels, tf.nn.softmax(predictions[2]))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, epochs=50, early_stopping_patience=5):\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Reset metrics for this epoch\n",
    "            self.reset_metrics()\n",
    "            \n",
    "            # Training loop\n",
    "            train_losses = []\n",
    "            with tqdm(self.train_dataset, desc=f\"Epoch {epoch + 1}/{epochs}\") as pbar:\n",
    "                for images, labels in pbar:\n",
    "                    loss = self.train_step(images, labels)\n",
    "                    train_losses.append(float(loss))\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f'{np.mean(train_losses):.4f}',\n",
    "                        'accuracy': f'{self.train_accuracy.result().numpy():.4f}'\n",
    "                    })\n",
    "            \n",
    "            # Calculate average training loss\n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            \n",
    "            # Validation loop\n",
    "            val_losses = []\n",
    "            for images, labels in self.val_dataset:\n",
    "                val_loss = self.val_step(images, labels)\n",
    "                val_losses.append(float(val_loss))\n",
    "            \n",
    "            # Calculate average validation loss\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            \n",
    "            # Get accuracies\n",
    "            train_acc = self.train_accuracy.result().numpy()\n",
    "            val_acc = self.val_accuracy.result().numpy()\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            self.history['val_loss'].append(avg_val_loss)\n",
    "            self.history['train_accuracy'].append(train_acc)\n",
    "            self.history['val_accuracy'].append(val_acc)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(\n",
    "                f\"\\nEpoch {epoch + 1}/{epochs}:\\n\"\n",
    "                f\"Train Loss: {avg_train_loss:.4f} - Train Accuracy: {train_acc:.4f}\\n\"\n",
    "                f\"Val Loss: {avg_val_loss:.4f} - Val Accuracy: {val_acc:.4f}\"\n",
    "            )\n",
    "            \n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                self.model.save_weights('best_kinship_model.h5')\n",
    "                patience_counter = 0\n",
    "                print(\"Saved new best model!\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered!\")\n",
    "                    break\n",
    "        \n",
    "        # Load best weights\n",
    "        self.model.load_weights('best_kinship_model.h5')\n",
    "        \n",
    "        # Plot training history\n",
    "        self.plot_history()\n",
    "    \n",
    "    def plot_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(self.history['val_loss'], label='Val Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history['train_accuracy'], label='Train Accuracy')\n",
    "        plt.plot(self.history['val_accuracy'], label='Val Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        all_losses = []\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        \n",
    "        for images, labels in tqdm(self.test_dataset, desc=\"Evaluating\"):\n",
    "            predictions = self.model(images, training=False)\n",
    "            loss = combined_loss(labels, predictions)\n",
    "            all_losses.append(float(loss))\n",
    "            \n",
    "            classification_logits = predictions[2]\n",
    "            test_accuracy.update_state(labels, tf.nn.softmax(classification_logits))\n",
    "            \n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(tf.argmax(classification_logits, axis=1).numpy())\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nTest Results:\")\n",
    "        print(f\"Test Loss: {np.mean(all_losses):.4f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy.result().numpy():.4f}\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_labels, all_predictions, \n",
    "                                 target_names=RELATIONSHIP_TYPES))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_predictions)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=RELATIONSHIP_TYPES,\n",
    "                   yticklabels=RELATIONSHIP_TYPES)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Splitting dataset...\n",
      "\n",
      "Dataset splits:\n",
      "Train: 132685 samples\n",
      "Validation: 28432 samples\n",
      "Test: 28433 samples\n"
     ]
    }
   ],
   "source": [
    "# Data loading and preparation\n",
    "print(\"Loading dataset...\")\n",
    "triplets_df = pd.read_csv('../data/processed/fiw/train/filtered_triplets_with_labels.csv')\n",
    "\n",
    "# Split dataset\n",
    "print(\"Splitting dataset...\")\n",
    "train_df, temp_df = train_test_split(\n",
    "    triplets_df, test_size=0.3, stratify=triplets_df['ptype'], random_state=42\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df['ptype'], random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Validation: {len(val_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 19:32:34.337687: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-10-27 19:32:34.337725: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-10-27 19:32:34.337735: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-10-27 19:32:34.337755: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-27 19:32:34.337770: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "print(\"\\nCreating datasets...\")\n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df)\n",
    "test_dataset = create_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model...\n"
     ]
    }
   ],
   "source": [
    "# Create model and trainer\n",
    "print(\"\\nInitializing model...\")\n",
    "model = KinshipModel()\n",
    "trainer = KinshipTrainer(model, train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2caeedbb3f044d0a86c9600c3c627f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train(epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinship_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

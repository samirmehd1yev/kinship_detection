{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SELECTED_RELATIONSHIP_TYPES = ['ss', 'bb', 'ms', 'fs', 'fd', 'md', 'sibs']\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = '/mimer/NOBACKUP/groups/naiss2023-22-1358/samir_kinship_data/processed/fiw/train'\n",
    "TRIPLETS_CSV = os.path.join(DATA_ROOT, 'filtered_triplets_with_labels.csv')\n",
    "OUTPUT_DIR = os.path.join(DATA_ROOT, 'splits')\n",
    "MODEL_DIR = os.path.join(DATA_ROOT, 'models')\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Load and Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triplets before filtering: 189550\n",
      "\n",
      "Initial relationship distribution:\n",
      "ptype\n",
      "ss      34230\n",
      "bb      34230\n",
      "ms      30874\n",
      "fs      25801\n",
      "fd      23708\n",
      "md      23321\n",
      "sibs    12131\n",
      "gfgs     1570\n",
      "gmgs     1368\n",
      "gfgd     1277\n",
      "gmgd     1040\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total triplets after filtering: 184295\n",
      "\n",
      "Final relationship distribution:\n",
      "ptype\n",
      "ss      34230\n",
      "bb      34230\n",
      "ms      30874\n",
      "fs      25801\n",
      "fd      23708\n",
      "md      23321\n",
      "sibs    12131\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample triplet:\n",
      "Triplet_ID                                                    1\n",
      "Anchor        data/processed/fiw/train/train-faces/F0001/MID...\n",
      "Positive      data/processed/fiw/train/train-faces/F0001/MID...\n",
      "Negative      data/processed/fiw/train/train-faces/F0361/MID...\n",
      "ptype                                                        fs\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the filtered triplets\n",
    "df = pd.read_csv(TRIPLETS_CSV)\n",
    "print(\"Total triplets before filtering:\", len(df))\n",
    "print(\"\\nInitial relationship distribution:\")\n",
    "print(df['ptype'].value_counts())\n",
    "\n",
    "# Filter out grandparent relationships\n",
    "df = df[df['ptype'].isin(SELECTED_RELATIONSHIP_TYPES)]\n",
    "print(\"\\nTotal triplets after filtering:\", len(df))\n",
    "print(\"\\nFinal relationship distribution:\")\n",
    "print(df['ptype'].value_counts())\n",
    "\n",
    "print(\"\\nSample triplet:\")\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Stratified Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_splits(df, train_size=0.7, val_size=0.15, random_state=42):\n",
    "    \"\"\"Create train/val/test splits while maintaining relationship distribution\"\"\"\n",
    "    \n",
    "    # First split: separate test set\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=TEST_SIZE,\n",
    "        stratify=df['ptype'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Second split: separate train and validation sets\n",
    "    relative_val_size = val_size / (train_size + val_size)\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=relative_val_size,\n",
    "        stratify=train_val_df['ptype'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Create splits\n",
    "train_df, val_df, test_df = create_stratified_splits(\n",
    "    df, \n",
    "    train_size=TRAIN_SIZE, \n",
    "    val_size=VAL_SIZE,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save Splits and Print Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split Sizes:\n",
      "Train set: 129005 triplets (70.0%)\n",
      "Validation set: 27645 triplets (15.0%)\n",
      "Test set: 27645 triplets (15.0%)\n",
      "\n",
      "Relationship Distribution in Each Split:\n",
      "\n",
      "Train set:\n",
      "ptype\n",
      "bb      23960\n",
      "ss      23960\n",
      "ms      21612\n",
      "fs      18061\n",
      "fd      16596\n",
      "md      16325\n",
      "sibs     8491\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set:\n",
      "ptype\n",
      "bb      5135\n",
      "ss      5135\n",
      "ms      4631\n",
      "fs      3870\n",
      "fd      3556\n",
      "md      3498\n",
      "sibs    1820\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set:\n",
      "ptype\n",
      "bb      5135\n",
      "ss      5135\n",
      "ms      4631\n",
      "fs      3870\n",
      "fd      3556\n",
      "md      3498\n",
      "sibs    1820\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save splits\n",
    "train_df.to_csv(os.path.join(OUTPUT_DIR, 'train_triplets.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(OUTPUT_DIR, 'val_triplets.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(OUTPUT_DIR, 'test_triplets.csv'), index=False)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Dataset Split Sizes:\")\n",
    "print(f\"Train set: {len(train_df)} triplets ({len(train_df)/len(df):.1%})\")\n",
    "print(f\"Validation set: {len(val_df)} triplets ({len(val_df)/len(df):.1%})\")\n",
    "print(f\"Test set: {len(test_df)} triplets ({len(test_df)/len(df):.1%})\")\n",
    "\n",
    "print(\"\\nRelationship Distribution in Each Split:\")\n",
    "print(\"\\nTrain set:\")\n",
    "print(train_df['ptype'].value_counts())\n",
    "print(\"\\nValidation set:\")\n",
    "print(val_df['ptype'].value_counts())\n",
    "print(\"\\nTest set:\")\n",
    "print(test_df['ptype'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for overlapping triplets...\n",
      "Train-Val overlap: 0\n",
      "Train-Test overlap: 0\n",
      "Val-Test overlap: 0\n",
      "\n",
      "Unique images in each split:\n",
      "Train: 14463\n",
      "Validation: 12657\n",
      "Test: 12655\n"
     ]
    }
   ],
   "source": [
    "# Check for overlapping triplets between splits\n",
    "train_triplets = set(train_df['Triplet_ID'])\n",
    "val_triplets = set(val_df['Triplet_ID'])\n",
    "test_triplets = set(test_df['Triplet_ID'])\n",
    "\n",
    "print(\"Checking for overlapping triplets...\")\n",
    "print(f\"Train-Val overlap: {len(train_triplets & val_triplets)}\")\n",
    "print(f\"Train-Test overlap: {len(train_triplets & test_triplets)}\")\n",
    "print(f\"Val-Test overlap: {len(val_triplets & test_triplets)}\")\n",
    "\n",
    "# Check unique images in each split\n",
    "def get_unique_images(df):\n",
    "    images = set()\n",
    "    for col in ['Anchor', 'Positive', 'Negative']:\n",
    "        images.update(df[col].unique())\n",
    "    return images\n",
    "\n",
    "train_images = get_unique_images(train_df)\n",
    "val_images = get_unique_images(val_df)\n",
    "test_images = get_unique_images(test_df)\n",
    "\n",
    "print(\"\\nUnique images in each split:\")\n",
    "print(f\"Train: {len(train_images)}\")\n",
    "print(f\"Validation: {len(val_images)}\")\n",
    "print(f\"Test: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create DataLoader Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample batch contents:\n",
      "triplet_id: shape=torch.Size([128]), dtype=torch.int64\n",
      "anchor_path: type=<class 'list'>, length=128\n",
      "positive_path: type=<class 'list'>, length=128\n",
      "negative_path: type=<class 'list'>, length=128\n",
      "relationship: shape=torch.Size([128]), dtype=torch.int64\n"
     ]
    }
   ],
   "source": [
    "class KinshipDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            'triplet_id': row['Triplet_ID'],\n",
    "            'anchor_path': row['Anchor'],\n",
    "            'positive_path': row['Positive'],\n",
    "            'negative_path': row['Negative'],\n",
    "            'relationship': SELECTED_RELATIONSHIP_TYPES.index(row['ptype'])\n",
    "        }\n",
    "\n",
    "# Test dataloader creation\n",
    "train_dataset = KinshipDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Test a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(\"\\nSample batch contents:\")\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: shape={v.shape}, dtype={v.dtype}\")\n",
    "    else:\n",
    "        print(f\"{k}: type={type(v)}, length={len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kinship Environment",
   "language": "python",
   "name": "kinship"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

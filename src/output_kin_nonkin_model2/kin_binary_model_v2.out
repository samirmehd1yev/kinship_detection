This job can be monitored from: https://job.c3se.chalmers.se/alvis/3202188
Validating 247192 image pairs...
Found 247192 valid pairs
Validating 52872 image pairs...
Found 52872 valid pairs
Validating 52152 image pairs...
Found 52152 valid pairs
Loading pretrained weights from /cephyr/users/mehdiyev/Alvis/kinship_project/src/pretrained_models/cosface_backbone_r50.pth

Analyzing architecture differences:

Missing keys in pretrained weights:
  layer1.0.channel_attention.0.bias: torch.Size([4])
  layer1.0.channel_attention.0.weight: torch.Size([4, 64])
  layer1.0.channel_attention.2.bias: torch.Size([64])
  layer1.0.channel_attention.2.weight: torch.Size([64, 4])
  layer1.0.spatial_attention.0.bias: torch.Size([1])
  layer1.0.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer1.1.channel_attention.0.bias: torch.Size([4])
  layer1.1.channel_attention.0.weight: torch.Size([4, 64])
  layer1.1.channel_attention.2.bias: torch.Size([64])
  layer1.1.channel_attention.2.weight: torch.Size([64, 4])
  layer1.1.spatial_attention.0.bias: torch.Size([1])
  layer1.1.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer1.2.channel_attention.0.bias: torch.Size([4])
  layer1.2.channel_attention.0.weight: torch.Size([4, 64])
  layer1.2.channel_attention.2.bias: torch.Size([64])
  layer1.2.channel_attention.2.weight: torch.Size([64, 4])
  layer1.2.spatial_attention.0.bias: torch.Size([1])
  layer1.2.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer2.0.channel_attention.0.bias: torch.Size([8])
  layer2.0.channel_attention.0.weight: torch.Size([8, 128])
  layer2.0.channel_attention.2.bias: torch.Size([128])
  layer2.0.channel_attention.2.weight: torch.Size([128, 8])
  layer2.0.spatial_attention.0.bias: torch.Size([1])
  layer2.0.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer2.1.channel_attention.0.bias: torch.Size([8])
  layer2.1.channel_attention.0.weight: torch.Size([8, 128])
  layer2.1.channel_attention.2.bias: torch.Size([128])
  layer2.1.channel_attention.2.weight: torch.Size([128, 8])
  layer2.1.spatial_attention.0.bias: torch.Size([1])
  layer2.1.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer2.2.channel_attention.0.bias: torch.Size([8])
  layer2.2.channel_attention.0.weight: torch.Size([8, 128])
  layer2.2.channel_attention.2.bias: torch.Size([128])
  layer2.2.channel_attention.2.weight: torch.Size([128, 8])
  layer2.2.spatial_attention.0.bias: torch.Size([1])
  layer2.2.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer2.3.channel_attention.0.bias: torch.Size([8])
  layer2.3.channel_attention.0.weight: torch.Size([8, 128])
  layer2.3.channel_attention.2.bias: torch.Size([128])
  layer2.3.channel_attention.2.weight: torch.Size([128, 8])
  layer2.3.spatial_attention.0.bias: torch.Size([1])
  layer2.3.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.0.channel_attention.0.bias: torch.Size([16])
  layer3.0.channel_attention.0.weight: torch.Size([16, 256])
  layer3.0.channel_attention.2.bias: torch.Size([256])
  layer3.0.channel_attention.2.weight: torch.Size([256, 16])
  layer3.0.spatial_attention.0.bias: torch.Size([1])
  layer3.0.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.1.channel_attention.0.bias: torch.Size([16])
  layer3.1.channel_attention.0.weight: torch.Size([16, 256])
  layer3.1.channel_attention.2.bias: torch.Size([256])
  layer3.1.channel_attention.2.weight: torch.Size([256, 16])
  layer3.1.spatial_attention.0.bias: torch.Size([1])
  layer3.1.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.10.channel_attention.0.bias: torch.Size([16])
  layer3.10.channel_attention.0.weight: torch.Size([16, 256])
  layer3.10.channel_attention.2.bias: torch.Size([256])
  layer3.10.channel_attention.2.weight: torch.Size([256, 16])
  layer3.10.spatial_attention.0.bias: torch.Size([1])
  layer3.10.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.11.channel_attention.0.bias: torch.Size([16])
  layer3.11.channel_attention.0.weight: torch.Size([16, 256])
  layer3.11.channel_attention.2.bias: torch.Size([256])
  layer3.11.channel_attention.2.weight: torch.Size([256, 16])
  layer3.11.spatial_attention.0.bias: torch.Size([1])
  layer3.11.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.12.channel_attention.0.bias: torch.Size([16])
  layer3.12.channel_attention.0.weight: torch.Size([16, 256])
  layer3.12.channel_attention.2.bias: torch.Size([256])
  layer3.12.channel_attention.2.weight: torch.Size([256, 16])
  layer3.12.spatial_attention.0.bias: torch.Size([1])
  layer3.12.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.13.channel_attention.0.bias: torch.Size([16])
  layer3.13.channel_attention.0.weight: torch.Size([16, 256])
  layer3.13.channel_attention.2.bias: torch.Size([256])
  layer3.13.channel_attention.2.weight: torch.Size([256, 16])
  layer3.13.spatial_attention.0.bias: torch.Size([1])
  layer3.13.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.2.channel_attention.0.bias: torch.Size([16])
  layer3.2.channel_attention.0.weight: torch.Size([16, 256])
  layer3.2.channel_attention.2.bias: torch.Size([256])
  layer3.2.channel_attention.2.weight: torch.Size([256, 16])
  layer3.2.spatial_attention.0.bias: torch.Size([1])
  layer3.2.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.3.channel_attention.0.bias: torch.Size([16])
  layer3.3.channel_attention.0.weight: torch.Size([16, 256])
  layer3.3.channel_attention.2.bias: torch.Size([256])
  layer3.3.channel_attention.2.weight: torch.Size([256, 16])
  layer3.3.spatial_attention.0.bias: torch.Size([1])
  layer3.3.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.4.channel_attention.0.bias: torch.Size([16])
  layer3.4.channel_attention.0.weight: torch.Size([16, 256])
  layer3.4.channel_attention.2.bias: torch.Size([256])
  layer3.4.channel_attention.2.weight: torch.Size([256, 16])
  layer3.4.spatial_attention.0.bias: torch.Size([1])
  layer3.4.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.5.channel_attention.0.bias: torch.Size([16])
  layer3.5.channel_attention.0.weight: torch.Size([16, 256])
  layer3.5.channel_attention.2.bias: torch.Size([256])
  layer3.5.channel_attention.2.weight: torch.Size([256, 16])
  layer3.5.spatial_attention.0.bias: torch.Size([1])
  layer3.5.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.6.channel_attention.0.bias: torch.Size([16])
  layer3.6.channel_attention.0.weight: torch.Size([16, 256])
  layer3.6.channel_attention.2.bias: torch.Size([256])
  layer3.6.channel_attention.2.weight: torch.Size([256, 16])
  layer3.6.spatial_attention.0.bias: torch.Size([1])
  layer3.6.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.7.channel_attention.0.bias: torch.Size([16])
  layer3.7.channel_attention.0.weight: torch.Size([16, 256])
  layer3.7.channel_attention.2.bias: torch.Size([256])
  layer3.7.channel_attention.2.weight: torch.Size([256, 16])
  layer3.7.spatial_attention.0.bias: torch.Size([1])
  layer3.7.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.8.channel_attention.0.bias: torch.Size([16])
  layer3.8.channel_attention.0.weight: torch.Size([16, 256])
  layer3.8.channel_attention.2.bias: torch.Size([256])
  layer3.8.channel_attention.2.weight: torch.Size([256, 16])
  layer3.8.spatial_attention.0.bias: torch.Size([1])
  layer3.8.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer3.9.channel_attention.0.bias: torch.Size([16])
  layer3.9.channel_attention.0.weight: torch.Size([16, 256])
  layer3.9.channel_attention.2.bias: torch.Size([256])
  layer3.9.channel_attention.2.weight: torch.Size([256, 16])
  layer3.9.spatial_attention.0.bias: torch.Size([1])
  layer3.9.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer4.0.channel_attention.0.bias: torch.Size([32])
  layer4.0.channel_attention.0.weight: torch.Size([32, 512])
  layer4.0.channel_attention.2.bias: torch.Size([512])
  layer4.0.channel_attention.2.weight: torch.Size([512, 32])
  layer4.0.spatial_attention.0.bias: torch.Size([1])
  layer4.0.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer4.1.channel_attention.0.bias: torch.Size([32])
  layer4.1.channel_attention.0.weight: torch.Size([32, 512])
  layer4.1.channel_attention.2.bias: torch.Size([512])
  layer4.1.channel_attention.2.weight: torch.Size([512, 32])
  layer4.1.spatial_attention.0.bias: torch.Size([1])
  layer4.1.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer4.2.channel_attention.0.bias: torch.Size([32])
  layer4.2.channel_attention.0.weight: torch.Size([32, 512])
  layer4.2.channel_attention.2.bias: torch.Size([512])
  layer4.2.channel_attention.2.weight: torch.Size([512, 32])
  layer4.2.spatial_attention.0.bias: torch.Size([1])
  layer4.2.spatial_attention.0.weight: torch.Size([1, 2, 7, 7])
  layer_norm.bias: torch.Size([512])
  layer_norm.weight: torch.Size([512])

Extra keys in pretrained weights:

Shape mismatches:
Missing keys after loading: 146
First few missing keys: ['layer1.0.channel_attention.0.weight', 'layer1.0.channel_attention.0.bias', 'layer1.0.channel_attention.2.weight', 'layer1.0.channel_attention.2.bias', 'layer1.0.spatial_attention.0.weight']
Successfully loaded pretrained weights

Epoch 1/20
